# Phased development playbook for ClaudeMatrix

**Ship a walking skeleton first, then grow it.** The most effective strategy for building a complex Claude Code plugin that bundles an MCP server, hooks, and Matrix protocol is to deliver the thinnest possible end-to-end slice of working functionality in Phase 1, then systematically widen each layer across subsequent phases. Research across plugin ecosystems (VS Code, Obsidian, Mattermost, Slack), messaging bridge projects (Matterbridge, mautrix), and the emerging Claude Code plugin ecosystem converges on one principle: **each phase must be independently useful**—a product, not a prototype. The MCP server should start with 1–3 tools on stdio transport, messaging should begin with file-based or Unix socket IPC using a Matrix-compatible JSON envelope, and the full Conduit/Tuwunel homeserver should arrive only once basic agent-to-agent messaging is proven. This incremental approach lets you validate assumptions at each boundary before committing to heavyweight infrastructure.

## How to phase a multi-component plugin build

The "walking skeleton" pattern—proven by rust-analyzer, Oso's VS Code extension, and the v0.dev clone—provides the strongest foundation for ClaudeMatrix. Phase 0 establishes project scaffolding, CI/CD, and the basic plugin lifecycle (load/unload). Phase 1 delivers the single most important vertical slice: one MCP tool that sends a message, one hook that receives it, and a trivial file-based transport connecting them. This vertical orientation matters. Horizontal layering (build the entire data layer, then the entire transport, then the UI) produces nothing usable until the final phase. Vertical slicing means every phase ships something a developer can test end-to-end.

LaunchDarkly's framework for incremental delivery captures this well: "Find ways to decompose new features into incremental shipments that provide coherent bits of customer value. Optimize your schedule around what is most uncertain." For ClaudeMatrix, the most uncertain element is whether inter-instance communication works reliably, so that's what Phase 1 proves.

A concrete phase map for ClaudeMatrix would follow five stages. Phase 1 (MVP) delivers **2–3 MCP tools** (`send_message`, `read_messages`, `list_agents`), a `SessionStart` hook for agent registration, and file-based or Unix socket transport—zero external dependencies. Phase 2 adds room-based routing, message history, and HTTP transport implementing a minimal Matrix Client-Server API subset. Phase 3 introduces a lightweight Rust homeserver (Tuwunel or Conduwuit) for persistent storage and proper room management. Phase 4 layers on encryption via `matrix-sdk-crypto` and structured agent identities. Phase 5 enables federation for cross-deployment communication. Each phase has clear go/no-go criteria, and each delivers standalone utility.

The "come for the tool, stay for the network" principle (articulated by Chris Dixon and Andrew Chen in the context of network effects) is critical here. Phase 1 must solve a real problem even when only one Claude Code instance is running—self-messaging for session persistence, memo functionality, or structured logging. This directly addresses the cold-start problem that kills messaging platforms before they gain traction.

## MCP server and Matrix protocol: building each layer incrementally

**MCP development follows a clear staircase.** The authoritative guidance from Anthropic's documentation, Phil Schmid's "MCP is Not the Problem, It's your Server" analysis, and Block's MCP Playbook all converge: start with **1–3 tools on stdio transport**, validate each with MCP Inspector, then expand. MCP servers are agent interfaces, not REST API wrappers—tools should be designed around outcomes, not operations. Instead of exposing `get_agent()`, `list_rooms()`, and `send_to_room()` as separate tools, a single `message_agent(recipient, content)` tool that handles discovery and routing internally produces better agent behavior.

The MCP Inspector is the primary testing tool at every phase. It runs on `localhost:6274` with tabs for Tools, Resources, and Prompts, and critically supports **CLI mode for CI/CD**: `npx @modelcontextprotocol/inspector --cli call-tool send_message '{"to": "agent-2", "content": "hello"}' node server.js` outputs JSON for automated pipelines. Beyond the Inspector, the official MCP client SDK enables automated testing with persistent connections mirroring real usage. Codely.com documents a test structure that mirrors API testing: one test file per tool, one per resource, one per prompt, with setup/teardown managing the MCP client connection lifecycle.

As the tool count grows beyond Phase 1, three scaling patterns emerge from production MCP servers. Up to ~30 tools, **namespace with forward-slash prefixes** (`messaging/send`, `messaging/read`, `rooms/create`). Beyond 30, adopt **dynamic toolset management** as GitHub's production MCP server does—loading only context-relevant tools at runtime via `ListAvailableToolsets()` and `EnableToolset()`. At enterprise scale, split into multiple focused servers by domain, permissions, or performance profile. For transport, start with stdio (maximum client compatibility) and graduate to **Streamable HTTP** (the replacement for SSE as of the MCP 2025-06-18 spec) when production deployment requires it.

**Matrix adoption follows its own staircase, starting well below a full homeserver.** The IETF Matrix Framework draft explicitly states that the specification is "quite large, yet modular"—you can implement only the portions you need. The critical design decision is to define a **Matrix-compatible JSON message envelope from day one**, even when the transport is trivial. An event with `type`, `content`, `sender`, `room_id`, and `timestamp` fields will work identically over a Unix socket in Phase 1 and over a federated homeserver in Phase 5.

The homeserver comparison data strongly favors the Conduit lineage for ClaudeMatrix. In matrix.org's Romeo & Juliet benchmark, Conduit completed the test in **4 seconds** versus Synapse's 5 minutes on SQLite. The current recommendation is Tuwunel (the Conduit → Conduwuit → Tuwunel succession chain), which is sponsored by the Swiss government, ships as a single Rust binary with an embedded database, and implements the full Matrix spec. It can run with **~500MB of database storage and negligible RAM/CPU**—light enough to bundle with a plugin.

The matrix-rust-sdk mirrors this incremental philosophy through its layered architecture. `matrix_sdk_crypto` is a standalone encryption state machine with no network I/O—you can add E2EE to any transport independently. `matrix_sdk_base` is a no-network-IO client state machine embeddable in existing network stacks. The full `matrix_sdk` adds connection management and high-level abstractions. This layering maps directly to ClaudeMatrix's phases: use your own transport with Matrix-shaped events first, then plug in `matrix_sdk_base` for state management, then graduate to the full SDK with a live homeserver.

## Claude Code plugin release mechanics and feedback loops

Claude Code plugins (launched October 2025) are shareable packages that bundle commands, agents, skills, hooks, MCP servers, LSP servers, and settings into a single installable unit. The plugin directory structure places `plugin.json` metadata inside `.claude-plugin/`, with all functional directories (commands, agents, skills, hooks) at the plugin root alongside `.mcp.json` for MCP server configuration. The **hooks system now covers 14 lifecycle events** including `SessionStart`, `PreToolUse`, `PostToolUse`, `Stop`, `SubagentStart`, `TeammateIdle`, and `TaskCompleted`, with three handler types: shell commands receiving JSON on stdin, single-turn LLM evaluations, and full subagent spawns with tool access.

For alpha/beta releases, the ecosystem provides a clear ladder. During active development, use `claude --plugin-dir ./my-plugin` for instant local testing without installation overhead. For alpha distribution, create a **private Git repository marketplace** and share it with select testers via `/plugin marketplace add owner/repo` with auth tokens. For team-level beta testing, configure the plugin in `.claude/settings.json` for automatic installation across the team. For wider release, submit to the official Anthropic plugin directory. Anthropic's own experimental plugins signal alpha status through **pre-1.0 versioning** (`hookify: 0.1.0`, `plugin-dev: 0.1.0`), a convention ClaudeMatrix should follow.

Version automation matters for a multi-phase project. The recommended toolchain is **Release Please** configured with `release-type: "simple"` and `extra-files` pointing to `plugin.json` via JSONPath for automatic version bumps. Combined with Conventional Commits (`feat:`, `fix:`, `BREAKING CHANGE:`), this generates changelogs, creates Git tags, and publishes releases automatically. For marketplace distribution, version pinning with `ref` (branch/tag) and `sha` (commit hash) ensures reproducible installations across all users.

Breaking changes between phases should follow the Terraform model: **deprecate in version N, remove no earlier than version N+2**. During the 0.x series, the SemVer spec permits anything to change—but practically, Aaron Stannard's "practical SemVer" principle applies: breaking changes must be cost-justified and accompanied by a public migration guide. Slack's evolution offers a model here—when they needed to rebuild their channel API, they created an entirely new Conversations API rather than breaking the old one, letting developers opt in when ready.

## Testing multi-agent messaging and mitigating rollout risks

**67% of multi-agent system failures stem from inter-agent interactions**, not individual agent defects (per Stanford AI Lab research). This statistic drives the testing strategy: individual agent testing is necessary but radically insufficient. A four-level testing framework from EY's Peter Capsalis, published in Real World Data Science, provides the structure.

Level 1 tests determinism and reproducibility—does each agent produce consistent output for identical inputs with fixed seeds? Level 2 tests context management—does the agent hallucinate outputs instead of calling tools, and does it handle tool errors gracefully? Level 3 tests inter-agent communication—do agents successfully hand off tasks, and does message format compatibility hold across agent boundaries? Level 4 tests system-level error propagation—what happens when one agent fails mid-conversation, and can the system detect and communicate null/failed outputs? For ClaudeMatrix specifically, Levels 3 and 4 are where most bugs will hide, and they require multi-instance test environments from Phase 1 onward.

The cold-start problem demands deliberate architectural countermeasures:

- Build standalone utility first—session persistence, structured logging, or memo functionality that works with a single Claude Code instance
- Create **echo agents or mock responders** that simulate a populated network during the bootstrap phase, following Reddit's strategy of seeding content to create initial momentum
- Use Google Cloud's synthetic data generation approach to create test agent interactions before real multi-agent deployment exists
- Design the Phase 1 `send_message` tool to degrade gracefully when no recipients exist, returning useful feedback rather than errors

Amazon's two-phase deployment technique from the AWS Builders' Library is the gold standard for rolling out protocol changes between phases. In the **Prepare phase**, all instances learn to read both old and new message formats while still writing the old format. After a bake period of several days confirming stability, the **Activate phase** switches writing to the new format. Each phase is independently safe to roll back because Prepare-phase code can still read Activate-phase data. Amazon DynamoDB engineers verified every single server at each phase boundary—ClaudeMatrix should verify every connected agent instance similarly.

For ongoing chaos testing as the system matures, inject network delays between agents (10ms–1000ms) to test timeout handling, randomly drop messages to verify retry logic and at-least-once delivery, kill agent processes mid-message to test recovery, and inject malformed messages to validate error handling. Contract testing—defining message schemas as formal contracts between agents and verifying both producer and consumer conformance—catches schema evolution bugs before they reach production.

## Architecture patterns from battle-tested projects

Ten transferable patterns emerge from studying rust-analyzer, Matterbridge, mautrix, Mattermost, Chromium, and production Discord bots. The most critical for ClaudeMatrix are the first three.

**"Thin wrapper, fat core"** (from rust-analyzer and Oso) means keeping protocol-specific code—MCP tool definitions, hook handlers, Matrix event serialization—in thin outer layers while the domain logic (message routing, agent discovery, room management) remains protocol-agnostic. rust-analyzer enforces this rigorously: only the outermost crate knows about LSP/JSON, and the `ide` crate's API is designed for a "hypothetical ideal client," not any specific protocol. For ClaudeMatrix, this means the message routing core should work identically whether the transport is a Unix socket or a federated Matrix homeserver.

**"Unified interface abstraction"** (from mautrix's bridgev2 rewrite) addresses the maintenance explosion that happens when multiple subsystems have slightly different interfaces. The mautrix ecosystem learned this the hard way after maintaining 8+ bridges with subtly different login flows and configuration formats. Their solution—a single `NetworkConnector` interface that all bridges implement—enabled unified testing, unified configuration, and a reusable management UI. ClaudeMatrix should define its transport abstraction interface before writing the first line of transport-specific code.

**"Graceful degradation over failure"** (from rust-analyzer) means computing `(T, Vec<Error>)` rather than `Result<T, Error>`. Each MCP tool call and hook invocation should be protected by catch_unwind or try-catch. A crash in the messaging subsystem should never bring down the MCP server or vice versa. rust-analyzer wraps every LSP request this way, ensuring that a panic in one feature doesn't affect others.

For repository structure, **strong consensus favors monorepo** for tightly-coupled multi-component plugins. The VS Code LSP extension pattern (`client/` + `server/` + shared root config) translates directly to ClaudeMatrix (`plugin/` + `mcp-server/` + `homeserver/` + `shared/`). Atomic commits across components mean that a breaking change in the message format and the corresponding fix in every consumer land in the same PR. Use GitHub Milestones (one per phase: "v0.1 - MVP", "v0.2 - Rooms", "v1.0 - Stable") with associated issues and PRs, release tags for each version, and a tiered documentation strategy: README (current capabilities), CHANGELOG (version history following Keep a Changelog format), ROADMAP (future phases), and migration guides for any breaking transition.

Mattermost's maturity labels provide the right expectation-setting framework: **Experimental → Beta → Stable**, with different testing and deployment expectations at each stage. ClaudeMatrix Phase 1–2 should be labeled Experimental, Phase 3–4 as Beta, and Phase 5 onward as Stable. This signals clearly to early adopters what level of reliability to expect and prevents premature production deployment of unvalidated subsystems.

## Conclusion

The research reveals that ClaudeMatrix's complexity—bundling an MCP server, hooks, and a Matrix homeserver—is manageable if each layer follows its own proven incremental path while sharing a common message abstraction. The non-obvious insight is that the **message envelope format, not the transport, is the foundational architectural decision**. Designing Matrix-compatible JSON events from day one means every subsequent transport upgrade (file → HTTP → homeserver → federation) is a plumbing change rather than a data model migration. The second key insight is that **standalone single-instance utility is not a compromise—it's a prerequisite**, both for solving the cold-start problem and for validating the plugin's core value proposition before network effects matter. Finally, the mautrix ecosystem's painful evolution from per-bridge bespoke code to a unified `NetworkConnector` interface demonstrates that investing in transport abstraction early prevents the maintenance burden that kills ambitious multi-component projects. Start with the walking skeleton, validate each phase against concrete go/no-go criteria, and resist the pull of the full Matrix feature set until simpler transports prove the architecture sound.